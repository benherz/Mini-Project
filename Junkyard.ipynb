{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML_Mini_Project Junkyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loop over CV-Search and visualization of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to find best parameters\n",
    "max_depth_cv = []\n",
    "min_samples_split_cv = []\n",
    "min_samples_leaf_cv = []\n",
    "n_estimators_cv = []\n",
    "progress_bar = tqdm(20)\n",
    "for _ in range(20):\n",
    "    forest_cv.fit(x_train, y_train)\n",
    "    max_depth_cv.append(forest_cv.best_params_['max_depth'])\n",
    "    min_samples_split_cv.append(forest_cv.best_params_['min_samples_split'])\n",
    "    min_samples_leaf_cv.append(forest_cv.best_params_['min_samples_leaf'])\n",
    "    n_estimators_cv.append(forest_cv.best_params_['n_estimators'])\n",
    "    progress_bar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize results\n",
    "fig, axs = plt.subplots(2, 2, figsize=(5, 4))\n",
    "\n",
    "axs[0, 0].hist(n_estimators_cv, bins=5, edgecolor='black')\n",
    "axs[0, 0].set_title('n_estimators')\n",
    "\n",
    "axs[0, 1].hist(min_samples_split_cv, bins=5, edgecolor='black')\n",
    "axs[0, 1].set_title('min_samples_split')\n",
    "\n",
    "axs[1, 0].hist(min_samples_leaf_cv, bins=5, edgecolor='black')\n",
    "axs[1, 0].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1, 1].hist(max_depth_cv, bins=5, edgecolor='black')\n",
    "axs[1, 1].set_title('max_depth')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selecting only most important features of previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest ###\n",
    "\n",
    "# Get feature importances with names \n",
    "importance = pd.DataFrame(forest_cv2.best_estimator_.feature_importances_)\n",
    "feature_names = pd.DataFrame(x_train.columns)\n",
    "feature_importances = pd.concat([importance, feature_names], axis = 1)\n",
    "\n",
    "# Name columns accordingly\n",
    "feature_importances.columns = [\"Importance\", \"Feature\"]\n",
    "rf_features = feature_importances.sort_values(by = \"Importance\", ascending = False)\n",
    "rf_features[0:10]\n",
    "\n",
    "# We will now train a Random Forest again, but only regard the top 10 features above\n",
    "# Select top 10 features\n",
    "selected_features = rf_features[0:10][\"Feature\"]\n",
    "# Create training subset containing only those features\n",
    "x_train_rf = x_train[selected_features]\n",
    "# Create test subset containing the same features\n",
    "x_test_rf = x_test[selected_features]\n",
    "\n",
    "# y_train, as well as y_test of course remain unchanged\n",
    "\n",
    "\n",
    "### ADA Boost ###\n",
    "\n",
    "# Get feature importances with names \n",
    "importance = pd.DataFrame(ada_cv.best_estimator_.feature_importances_)\n",
    "feature_names = pd.DataFrame(x_train.columns)\n",
    "feature_importances = pd.concat([importance, feature_names], axis = 1)\n",
    "\n",
    "# Name columns accordingly\n",
    "feature_importances.columns = [\"Importance\", \"Feature\"]\n",
    "ada_features = feature_importances.sort_values(by = \"Importance\", ascending = False)\n",
    "ada_features[0:10]\n",
    "\n",
    "# We will now train a Random Forest again, but only regard the top 10 features above\n",
    "# Select top 10 features\n",
    "selected_features_ada = ada_features[0:10][\"Feature\"]\n",
    "# Create training subset containing only those features\n",
    "x_train_ada = x_train[selected_features_ada]\n",
    "# Create test subset containing the same features\n",
    "x_test_ada = x_test[selected_features_ada]\n",
    "\n",
    "# y_train, as well as y_test of course remain unchanged\n",
    "\n",
    "\n",
    "### Gradient Boost ###\n",
    "\n",
    "# Get feature importances with names \n",
    "importance = pd.DataFrame(xgb_cv.best_estimator_.feature_importances_)\n",
    "feature_names = pd.DataFrame(x_train.columns)\n",
    "feature_importances = pd.concat([importance, feature_names], axis = 1)\n",
    "\n",
    "# Name columns accordingly\n",
    "feature_importances.columns = [\"Importance\", \"Feature\"]\n",
    "xgb_features = feature_importances.sort_values(by = \"Importance\", ascending = False)\n",
    "xgb_features[0:10]\n",
    "\n",
    "# We will now train a Random Forest again, but only regard the top 10 features above\n",
    "# Select top 10 features\n",
    "selected_features_xgb = xgb_features[0:10][\"Feature\"]\n",
    "# Create training subset containing only those features\n",
    "x_train_xgb = x_train[selected_features_xgb]\n",
    "# Create test subset containing the same features\n",
    "x_test_xgb = x_test[selected_features_xgb]\n",
    "\n",
    "# y_train, as well as y_test of course remain unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature importance of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "fig = plt.figure()\n",
    "ax = fig.gca() #get current axis\n",
    "ax.bar(range(x_train.shape[1]), forest_cv2.best_estimator_.feature_importances_)\n",
    "ax.set_xticks(np.arange(x_train.shape[1]))\n",
    "ax.set_xticklabels([f'{col}' for col in x_train.columns])\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Feature Importance')\n",
    "ax.set_title('Random Forest Regression Feature Importances')\n",
    "\n",
    "# Ugly ass graph but its working "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
